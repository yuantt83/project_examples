{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing code for running on AWS SageMaker.\n",
    "We have uploaded our training and test data to AWS S3 bucket:  galaxyimages/training, galaxyimages/test\n",
    "Test the code on a  vanilla CNN first, before training the deep CNN from tranfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "\n",
    "sage_maker_session = sagemaker.Session()\n",
    "bucket = sage_maker_session.default_bucket()\n",
    "prefix = 'sagemaker/deep-galaxy-training'\n",
    "\n",
    "key = 'galaxyimages_mirror'\n",
    "train_input_path = 's3://{}/{}/training/'.format(bucket, key)\n",
    "\n",
    "print(bucket)\n",
    "print(role)\n",
    "print(train_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to put the customised code in a container - a Docker, otherwise SM cannot load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_vanilla_cnn.py\n",
    "# ok no modules called keras..\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    print(os.environ['SM_CHANNEL_TRAINING'])\n",
    "    print(os.environ['SM_CHANNEL_VALIDATION'])\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.01)\n",
    "    parser.add_argument('--batch-size', type=int, default=32)\n",
    "    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--training', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    epochs = args.epochs\n",
    "    lr = args.learning_rate\n",
    "    batch_size = args.batch_size\n",
    "    gpu_count = args.gpu_count\n",
    "    model_dir = args.model_dir\n",
    "    training_dir = args.training\n",
    "    validation_dir = args.training\n",
    "    test_dir = args.test\n",
    "    \n",
    "    num_classes = 3\n",
    "    image_resize = 200\n",
    "    batch_size_training = batch_size\n",
    "    batch_size_validation = batch_size\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       rotation_range=140,\n",
    "                                       horizontal_flip=True,\n",
    "                                       vertical_flip=True)\n",
    "\n",
    "    training_set = train_datagen.flow_from_directory(training_dir,\n",
    "                                                     target_size=(image_resize, image_resize),\n",
    "                                                     batch_size=batch_size_training,\n",
    "                                                     seed=100,\n",
    "                                                     subset='training',\n",
    "                                                     shuffle=False,\n",
    "                                                     class_mode='categorical')\n",
    "\n",
    "    STEP_SIZE_TRAIN = training_set.n // training_set.batch_size\n",
    "    \n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                       validation_split=0.12)\n",
    "\n",
    "    valid_set = valid_datagen.flow_from_directory(training_dir,\n",
    "                                                  target_size=(image_resize, image_resize),\n",
    "                                                  batch_size=batch_size_validation,\n",
    "                                                  seed=100,\n",
    "                                                  subset='validation',\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')\n",
    "    STEP_SIZE_VALID = valid_set.n // valid_set.batch_size \n",
    "    \n",
    "    cnn = models.Sequential()\n",
    "    cnn.add(layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[image_resize, image_resize, 3]))\n",
    "    cnn.add(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "    cnn.add(layers.Conv2D(filters=36, kernel_size=3, activation='relu'))\n",
    "    cnn.add(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "    cnn.add(layers.Flatten())\n",
    "    cnn.add(layers.Dense(units=128, activation='relu'))\n",
    "    cnn.add(layers.Dropout(0.5))\n",
    "    cnn.add(layers.Dense(units=num_classes, activation='softmax')) \n",
    "    cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    print(cnn.summary())\n",
    "    \n",
    "    fit_results = cnn.fit(x=training_set,\n",
    "                          steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                          validation_data=valid_set,\n",
    "                          validation_steps=STEP_SIZE_VALID,\n",
    "                          epochs=epochs\n",
    "                          )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_version = tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "ON_SAGEMAKER_NOTEBOOK = True\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "if ON_SAGEMAKER_NOTEBOOK:\n",
    "    role = sagemaker.get_execution_role()\n",
    "else:\n",
    "    role = 'your_role'\n",
    "\n",
    "train_instance_type = 'ml.m5.large'      # The type of EC2 instance which will be used for training\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "                          entry_point='train_vanilla_cnn.py',          # our own script\n",
    "                          role=role,\n",
    "                          framework_version='2.3.0',              \n",
    "                          hyperparameters={'epochs': 3,\n",
    "                                           'batch_size': 32\n",
    "                                           },\n",
    "                          py_version='py37',\n",
    "                          script_mode=True,\n",
    "                          instance_count=1,                 \n",
    "                          instance_type=train_instance_type\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_path = 's3://{}/{}/training/'.format(bucket, key)\n",
    "print(train_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training ...\")\n",
    "tf_estimator.fit({'training': train_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "endpoint_name = 'galaxyimages'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "end_point = tf_estimator.deploy(initial_instance_count=1,instance_type='ml.m5.4xlarge',endpoint_name=endpoint_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deploying ...\")\n",
    "predictor = tf_estimator.deploy(initial_instance_count=1, instance_type=deploy_instance_type)\n",
    "\n",
    "print(\"Predictor endpoint name : %s\" % predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model performance\n",
    "from sagemaker.tensorflow.serving import Predictor\n",
    "import numpy as  np\n",
    "\n",
    "\n",
    "test_input_path = \"s3://{}/{}/test/\".format(bucket, key)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory(test_input_path,\n",
    "                                            target_size = (200, 200),\n",
    "                                            batch_size = 32,\n",
    "                                            seed = 10, \n",
    "                                            shuffle = False,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "test_set.reset()\n",
    "\n",
    "Y_pred = model_check.predict(\n",
    "    test_set,\n",
    "    steps=test_set.n / test_set.batch_size,\n",
    "    verbose=1)\n",
    "\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "\n",
    "# Predict\n",
    "predictor = Predictor(endpoint_name = predictor.endpoint)\n",
    "Y_pred = predictor.predict(test_set)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_set.classes, y_pred)\n",
    "\n",
    "print('The confusion matrix is \\n{}\\n'.format(cm))\n",
    "\n",
    "f1 = classification_report(test_set.classes, y_pred, target_names = test_set.class_indices)\n",
    "print('F1 score is {}\\n'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean  up by Deleting Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
